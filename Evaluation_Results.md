# ANALYZING PERFORMANCE

To pick a machine learning model out of a bunch of classifiers, we need to assess different performance parameters and the kind of data that we are working with. Performance of
the models have been evaluated on the following parameters:

1. ROC curve and AUC value â€“ The ROC curve displays the TPR against the FPR at different threshold levels, separating the signal from the noise. The Area Under the Curve is used to measures the classifier's ability to distinguish between classes.
2. RMSE - The root-mean-square error is a metric used for comparing values predicted by a model or estimate to values observed.
3. F-1 score - F1 Score is the weighted average of Precision and Recall. It is frequently more valuable than accuracy, especially if the distribution of classes is unequal.
4. Misclassification Rate - The Misclassification Rate is a performance indicator that indicates the percentage of incorrect predictions.
5. AIC Score - The Akaike information criterion is a predictor of prediction error and thus relative model quality for a given set of data.
6. Matthews Correlation Coefficient - MCC is a quality indicator for binary classifications.
